%!TEX root = ../Masterthesis.tex
\chapter{System conception}\section{Technical conception}
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/Technical_Workflow.png}
\caption{Technical conception of the single hardware parts}
\label{fig:technical-copnception}
\end{figure} 
As depicted in Figure \ref{fig:technical-copnception}, the technical components of the setup are relatively simple.Instead of using one large dimensioned unit which takes care of all calculations, the image processing steps that have to be done on both stereo images anyway, are outsourced onto the Raspberry Pi's.
The image processing can be done on the two Pi's in parallel, which cuts down processing time. Furthermore the image data stay on the device and does not have to be sent to a min unit which would introduce network and data reading latency.\\
The two \textbf{Raspberry Pi 3 Model B}, which are used as the controllers for the \textbf{Raspberry Pi Camera} are connected via network cable to a network switch. A connection over wireless network would be possible with the on-board capabilities, but this might create latency problems and/or signal interference with other existing networks. Therefor a cable connection is the safer solution.
The switch also connects to the "Master" PC to which the "Slave" Pi's communicate their data. The Master also takes care of the stereoscopic calculations,as it is dimensioned with far more computational power than the slaves. The 2D positional data from the slaves and the calculation results from the stereo image disparity is fed into the hand model running on the "Master". The model solution is then applied to the digital hand model and rendered.
\newpage
\section{Construction Details}
For the system construction, it is important to know what the primary use case of the system will be. A mobile system is principally not impossible as the Raspberry Pi's could be run with battery power. A wireless communication leads to the beforehand described problems and a long enough cable connection is rather cumbersome. Furthermore the cameras need to be calibrated at a fixed position for correct depth measurements. Some kind of mounting harness would be needed where the cameras stay in a fixed position.\\
A much easier approach is defining a seated use-case for the user. Since the systems main chore is to track the hand of the user, a seated position reduces position tracking errors as one does not have to take account large positional changes of the rest of the body.
The tracking volume that needs to be accomplished for a seated position is also reduced by our physical limitations of the arm length. A tracking volume of about $100x100x100$ cm should be sufficient to track most of the movement.\\
A frame of 30x30 mm construction Profile is used as base to mount all the components of the system. To reduce possible color reflections from the background of the tracking space, black fabric is used as background material. The other components of the system are:
\begin{itemize}
\item 2 Raspberry Pi 3 Model 3 with 1GB RAM and a 64GB micro-SD Card running a current Raspbian OS and a C++14 compiler
\item Compiled version of OpenCV and Boost library
\item Java LWJGL 3 and CALIKO library
\item 2 Raspberry Pi Camera Modules V2 with a 100 cm connector cable
\item Ethernet Switch
\item Monitor for calibration and debugging of the Raspberry side software
\item PC with decent graphics card and a current Java version 8+
\item Mounting peripherals
\item Active cooled case for the Raspberry
\item Two micro USB power supplies with 3.0 A output current
\item HTC Vive Lighthouse and Tracker
\end{itemize}
The active cooled Raspberry pi cases are an optional part of the construction, but the real time image processing on the CPU of the Raspberry together with the running camera produces larger amounts of heat than in normal operation mode.The applied heat sinks and the active cooling fan help with faster heat dissipation. The 3.0A power supply units are recommended as lower output currents still work but can lead to a power brown out on the system. This can cause system damage over longer usage times and even make the used SD card unreadable.
\subsubsection{Mounting peripherals}
The used cameras are basically camera sensors mounted on a PCB with an optic. They have 4 drilled holes as fixations points. A simple holder plate was designed and printed with a 3D printer for faster prototyping. The holder was designed as two parts to be able to change the mounting distance of the camera setup from the construction profile if needed.Cut-outs in the mounting plate for the cameras provide the ability to mount the cameras with two machine screws and move to end position of the cameras along a fixed axis (see Figure \ref{img:camera_holder}). This ensures that the cameras are mounted without a vertical position offset.For the current setup the distance between the two cameras is set to 75 mm which corresponds to the inter-axial distance of the human eyes.
\begin{figure}[H]
\includegraphics[height=\textwidth, angle=90]{images/CAmera_holder_2.JPG}
\caption{Technical drawing of the camera holder}
\label{img:camera_holder} 
\end{figure}
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/real_camera_setup.jpg}
\caption{Printed camera holder with mounted cameras}
\label{img:camera_real} 
\end{figure}
\section{Image analysis on the Raspberry}
For the image analysis part, \textit{OpenCV3} with it's \textit{Python 3} bindings is chosen. The \textit{OpenCV} package needs to be downloaded and compiled onto each device before it can be used.
Before the actual image analysis part can take place, the used cameras need to be calibrated. As they are basically pinhole cameras, they introduce amounts of radial and tangential distortion to the images. These distortions need to be compensated for, especially when using them as source for the stereo images. Image distortion in these pictures would lead to incorrect calculations for image depth.
\textit{OpenCV} supplies the tools to calculate the distortion parameters as well as the camera matrix \cite{Opencv.2018}.
The camera matrix is basically a description for the transformation of points in 3D object space to the 2D image space of the camera. For the used Cameras, we can consider a central projection of points in space onto our image plane. The center of the camera is considered the center of an euclidean coordinate system and the projection plane \textbf{z} is located at distance \textit{f} equal to the focal length of the camera.
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/pionhole.JPG}
\caption{Image point projection for pinhole camera systems\cite{Hartley.2000}}
\label{pinholecamera_mapping} 
\end{figure}
As shown in Figure \ref{pinholecamera_mapping}, a point from the object space can be projected onto the image plane by drawing a ray from the object space point to the camera center. The intersection point of the ray with the image plane defines the projection point. With this knowledge the projection can be described as :
\begin{equation}
(X,Y,Z)^{T} \mapsto (fX/Z,fY/Z)^{T}
\end{equation}
For the calculation of the image distortion , \textit{OpenCV} uses a chessboard image for calibration\cite{Opencv.2018b}. The printed image is held in front of the camera at a constant distance and rotated and positioned differently. After every position/rotation change an image of the pattern should be taken.
These images are then used to find the corners at the intersection of the black and white squares. With the previous knowledge of the square sizes, the projection errors in radial and tangential directions can be calculated:
\begin{equation}
\begin{split}
x_{tanDistorted}&=x(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}+r^{6})\\
y_{tanDistorted}&=y(1+k_{1}r^{2}+k_{2}r^{4}+k_{3}+r^{6})\\
\\
x_{radDistorted}&=x+[2p_{1}xy+p_{2}(r^{2}+2x^{2})]\\
y_{radDistorted}&=x+[p_{1}(r^{2}+2y^{2})+2p_{2}xy]\\
\end{split}
\end{equation} 
As can bee seen from the above equations, the parameters that need to be calculated are $k_{1},k_{2},p_{1},p_{2},k_{3}$ by the algorithm. These parameters are called the distortion coefficients. Together with the intrinsic and extrinsic parameters of the camera, the camera images can be distorted to get better image results. This undistortion operation has to be done on every image frame that the camera produces.\\
When using a stereoscopic setup, the cameras have to be stereo rectified to account for vertical deviation between the camera images.\textit{OpenCv} also provides means for doing so. Instead of taking images with each camera separately, both cameras are triggered simultaneously to take an image of the single chessboard used for camera calibration. With this set of image pairs, the calibration utility is able to calculate a remapping transformation for each camera. After the application of this remapping, corresponding image points should be on the same horizontal level in the image.\\After the cameras are calibrated, the resulting image optimization can be applied after image retrieval.\\The rest of the process is now comprised of:
\begin{itemize}
\item Image conversion to HSV colorspace
\item Mask construction and filtering
\item Contour finding an position calculation
\item Sending of positional data via network as UPD package
\end{itemize}
\subsection{Image acquisition from camera}
As the computation times for image filtering and mask generation may vary, it makes sense to separate the image acquisition from the computation part.Therefore the loading of the image data frame from the camera could be outsourced into its own thread. The synchronicity of the two frame reading threads on the Pi's is assumed for the first implementation and has to be tested on a finished setup.These treads will take the data from the current image frame on the camera and hand it over to the main thread where the image processing is handled. For the first frames, the full image data is needed since the position of the markers is not known. Performance-wise, this introduces a lot of overhead, since each mask generation step for the specific colors hast to go through the complete image data, meaning every pixel has to be read out at least 5 times to get the tracking for all different markers.
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/ROI.jpg}
\caption{ROI calculation for consecutive frames}
\label{roi_calc} 
\end{figure}
After a few frames, we get hold of the marker position and can define a reduced region of interest (ROI) on the image (Figure \ref{roi_calc}). The new ROI results from the positional data of the previous frame and has to be dimensioned correctly as to incorporate the possibility of large position differences between consecutive frames.
Furthermore, the offset position, preferably the top left corner of the region, has to be kept track of. By taking a sub-region of the frame as input for further processing, the resulting coordinates relate to the top left corner of this sub-region as origin. Consequently only the first set of coordinates has the correct origin as reference point for the next frame. Consecutive frames need this information to retrieve the correct sub-region from the full frame by adding the offset to the calculated positional data.
First frame ROI calculation is based on a minimum \textit{axis-aligned bounding box (AABB)} fitted onto the marker. The offset value for the ROI is added to the bounding box values resulting in a rectangle data-set containing the position of the offset top-left corner $\vec{\mathbf{tl}}$ and the offset 
\textbf{width} and \textbf{height} of the ROI rectangle:\\
\begin{equation}
\begin{split}
\vec{\mathbf{tl_{0}}}&=(AABB_{x_{0}}-\text{offset}_{x},AABB_{y_{0}}-\text{offset}_{y})\\
\mathbf{width_{0}}&=AABB{w_{0}}+(2\cdot\text{offset}_{x})\\
\mathbf{height_{0}}&=AABB_{h_{0}}+(2\cdot\text{offset}_{y})\\
\end{split}
\end{equation}\\
These values are taken to extract the sub-region of the consecutive at $\vec{\mathbf{tl}}$ with size of width and height. The $\vec{\mathbf{tl}}$ value is also saved as offset value $\vec{\mathbf{OffPos}}$ .
The position calculation for the proximate frame will utilize the offset value as follows:
\begin{equation}
\begin{split}
\vec{\mathbf{tl_{1}}_{x}}&=x_{1}+\vec{\mathbf{OffPos}}_{x}-\text{offset}_{x} \\ \vec{\mathbf{tl_{1}}_{y}}&=y_{1}+\vec{\mathbf{OffPos}}_{y}-\text{offset}_{y}\\
\mathbf{width_{1}}&=AABB_{w_{1}}+(2\cdot\text{offset}_{x})\\
\mathbf{height_{1}}&=AABB_{h_{1}}+(2\cdot\text{offset}_{y})\\
\end{split}
\end{equation}
This should result in the correct positional data for all frames following the initial frame.
Finally,the resulting rectangle has to be fitted onto the size boundaries of the full image frame. When applying a fixed offset value, the resulting region that will be extracted from the following frame might reach outside the size boundaries of the full frame. This can happen when the markers are positioned near the edges of the frame. Trying to read out pixels outside the available size of the image will without doubt result into an error by reason of unavailable data at these points. Therefore the combination of position width, height and offset has to be clamped to fit into the available image data:
\begin{equation}
\begin{split}
\textbf{width}&=\text(max(0,min(\mathbf{width},image_{width}))\\
\textbf{height}&=\text(max(0,min(\mathbf{height},image_{height}))
\end{split}
\end{equation}
Should no marker be found in the defined ROI or the certainty of the result is not high enough, the frame has to be dropped and the next frame should utilize the whole image data.
\subsection{Image conversion to HSV colorspace}
The image data that is supplied by the camera comes in an RGB data format, which we could already use for the further calculation. It does though make more sense to convert the input data into the HSV colorspace. Since we are not using high precision cameras, it is necessary to define a range of color values around the desired color which we want to track. The HSV colorspace is displayed as a cone, in comparison to the RGB colorspace, which is displayed as a cube. The color values for the HSV space are all located on a circle spanning form 0 to 360 degrees. The Hue value (H)corresponds to the angle on the circle, where $0^\circ$ corresponds to a reddish color, $120^\circ$ lies in the area of green and $240^\circ$ and above correspond to blue. The saturation value (s) corresponds to the amount of white the color contains  where 0 is pure white and 100 corresponds to the fully saturated color. For optimal results, only highly saturated colors should be used to ensure correct color detection. The last component is the value component (V) which describes the intensity of the color. Alike the value settings for the saturation, value ranges of at leas 50 should be used for tracking precision.
For tracking the five fingers of the hand we need five distinguishable colors. Here the primary colors red, green and blue will be the choice for the first three colors. The other two selected colors are orange and yellow. To be able to clearly distinguish these colors in the video frame, a constant and homogeneous lighting is needed as well as a color temperature of the lighting that is in the neutral area to not change the color of the markers.
\\The color conversion from the input RGB values to the desired HSV colorspace is done as follows:
\begin{equation}
\begin{split}
hsv_{low}&=(hue_{targetcolor}-sensitivity,100,100)\\
hsv_{up}&=(hue_{targetcolor}-sensitivity,100,100)\\
\end{split}
\end{equation}
\subsection{Mask construction and filtering}
\todo {orig image and mask images for display}
The HSV color converted values are the used as the parameters for a binary mask generation on the current frame. In this mask all pixels who's values lie outside of the specified range are set to zero (black) and the remaining are set to 255 (white).For these masks to work properly, any other larger objects that might contain similar colors should be removed from the scene to eliminate wrong tracking data. \\The used cameras run on an automated white balance and exposure mode. The variation of these values can cause shifts in the appearance of the colored markers and therefore alter the generated masks. To compensate for white balance shifting, a non-white ideally diffuse reflecting background should be used for the tracking space. Also the auto modes should be turned off and appropriate values for white balance and exposure have to be determined at the initialization step of the system. As all digital cameras tend to have signal noise in the sensor data, high frequency noise in the color channels will be present.To reduce the base noise level, a good lighting situatuon should be provided. Low level light situations require a higher signal gain .While this produces a brighter image, it also amplifies the base signal noise of the camera. The remaining noise should be filtered out before any further computation on the image data can be done.\\
The first step in this progress is to use a \textit{Gaussian filter} to blur the mask.
The \textit{Gaussian filter} acts as a low-pass filter for the image.The downside of applying a blur onto the generated image is a loss of detail. As the markers will not be utilizing complex geometric forms or fine patterns, the loss of details is acceptable.\\
After this step a combination of two morphological operation, erosion and dilation, is used to further improve the data of the thresholded binary image\cite[chapter~3.11-12]{Davies.2017}.
Erosion and dilation operation on thresholded images perform the tasks of removing light spots on dark backgrounds as well as dark spots on light backgrounds. These spots are usually the result of the remaining image noise after the blurring or incorrect pixel values from the camera which case the pixel to be incorrectly thresholded.\\
To apply the specific operators to the image a mask ( often also called kernel) needs to be defined. These masks normally consist of an n x n shaped matrix where the values in the matrix rows and columns are either one or zero for binary images. An odd number of rows and columns is usually used to be able to define a center pixel. The central entry of the matrix corresponds to the current pixel of the image for which the morphological operation is to be applied. As the data of the pixel should be preserved, the value is set to one. 
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/morpological_kernels.jpg}
\caption{ Example Mask of 3x3 size :\textbf{a)} empty mask \textbf{b)} Mask that keeps pixel value  \textbf{c)} combination of b) with a left shift   \textbf{d)} combination of b) with an up-shift   \textbf{e)} isotropic mask}
\label{morphological_ops} 
\end{figure}
in the further processing, the central pixel value and the values of the neighboring pixels are taken and summed up.The ones in the matrix outside of the center value define if the pixel is taken in for the operation. A threshold value is then defined. The result of the sum operation is compared to the threshold value. If above the threshold, the central pixel value is altered, depending on the image operation. If below, the current value is kept.
\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwData{Threshold}{threshVal}
\SetKwData{Result}{result}
\SetKwData{CentVal}{centVal}
\SetKwData{OpVal}{opVal}
\Input{threshold $threshVal$ value for the calculation, image segment $imgSegment$ of size $n\times n$, mask $morphMask$ of size $n \times n$, operation value (0 or 1) $opVal$, value of central pixel $centVal$}
\BlankLine
\KwResult{New value for pixel}
\BlankLine
\For{$i\leftarrow 0$ \KwTo maskHeight}{
	\For{$j\leftarrow 0$ \KwTo maskWidth}
	{
	 \If{$morphMask[i,j] = 1$}{\Result$\leftarrow$\Result$ +morphMask[i,j]$}
	}
}
\If{\Result $>=$ \Threshold}{\CentVal$=$ \OpVal}
\caption{Pseudocode for the the pixel value calculation}
\label{mask_calc_pseudocode}
\end{algorithm}

When using the isotropic matrix displayed in Figure \ref{morphological_ops} e) and setting a threshold of 8 on an dilation operation, the result would be the filling of a white pixel when surrounded by black pixel. The same operation on an erosion would lead to an removal of a black pixel surrounded by white pixels.\\
The combination of the morphological operator erode and dilate leads to two methods called \textit{morphological opening} and \textit{morphological closing}. The idea behind these two is to use the two basic operation in a certain order to enhance the binary image quality \cite[chapter~3.12]{Davies.2017}.
The formal definition for the operation:
\begin{equation}
 \begin{split}
O&=Original\; image \\
\oplus&= Dilation\; operation\\
E&=Erosion\; mask\\
\ominus&= Erosion\; operation\\
D&=Dilation\; mask\\
\circ&= Morphologic\; opening\\
\bullet&= Morphologic\; closing\\
\\
O\bullet D&=(O\oplus D)\ominus E\\
O\circ D&=(O \ominus E)\oplus D\\
\end{split}
\end{equation}

The \textit{morphological closing} operation is applied first to eliminate so called "salt noise" (white pixels in black area), narrow cracks, channels and small holes in the original image. The major part of the work for this operation is done on the areas where not tracking marker is found. It removes the remaining noise left over from the blur or signal distortion.\\
The application of the \textit{morphological opening} removes so called "pepper noise", fine hairs and small protrusions. The outcome of the practical application of the opening is the removal of wrong pixels inside the area of the color marker in the image. This helps with the later calculation of the bounding boxes as the algorithm searches for the largest closed area int the image.\\
Generally the closing of an image enlarges it while filling bright defects inside of an black area and the opening makes it smaller while removing bright defects on a dark surface.
As the system will not be handling large image data, the mask sizes can be kept low, utilizing the above described $3\times 3$ mask size. The structure options provided by \textit{OpenCV} contain a cross structure, an elliptical structure and a isotropic version. As it would be useful to get all surrounding image data and the tracking markers will not have complex geometric structures, the isotropic mask structure should be sufficient.
\subsection{Contour finding and position calculation}
With the cleaned mask we can continue and search for the white areas in the mask which might represent our target. Under the assumption that we have removed all other parasitic objects from the image, the marker should be the largest area of positive pixels in the mask frame. Therefore only the largest area found in the search is taken as the desired tracking marker. For the selected area, a fitting bounding box is calculated and the center of this box is used as the positional parameter of the tracking marker.
\subsection{object tracking}
For the object tracking part, a setup consisting of an \textit{HTC Vive Tracker} and a \textit{Lighthouse Basestation} is chosen.Optical tracking via the camera system would be possible, but each object would require a further distinguishable color marker. The color marker also needs to be calibrated for before the system can track it. The HTC system uses infrared signals to measure the position of the marker in space in relation to the position of the lighthouse. These do not interfere with the camera image as the cameras have a built-in infrared filter. This system setup makes it possible to quickly change the tracked object used without the need for re-calibration. Only the tracker position in relation to the Hand tracking coordinate system has to be set in the initial start up phase.  
\section{Systems communication}
The data that is generated on the two Raspberry's has to be communicated to the "Master" unit where further processing is done. The data delay between the different endpoints should be minimal. As the data is streamed in real time, protocols like TCP, which utilize control mechanisms for package order and receiving of data use to much overhead. In the time-frame needed for the protocol to request a missing package again, the data will already be obsolete. \\Furthermore, the amount of data that is send in each data frame is minimal, as this is only the position value of the specified number of tracking points and some separators for string processing. Since the devices will be connected to a local network via a switch, network workload from other devices and protocols can be assumed minimal and not impacting transport time.\\\begin{figure}[H]
\includegraphics[width=\textwidth]{images/Network_Diagram.png}
\caption{Work-flow for the UDP Data packages that are send from the Raspberry's to the parent PC}
\label{img:netzwerk_diagram} 
\end{figure}
Regarding all these points a simple UPD protocol setup seems to be the best choice. Package loss that might appear with UDP is not critical as it means only the loss of one frame data-set.\\
It has to be ensured that the UDP data streams from both devices are recorded separately to ensure that the data can be separated correctly for the stereoscopic calculations.
The possibility to send multi- and/or broadcast messages via UDP also open up an easy way of communication between the "Master" and the "Slave" Pi's for control communication.

\section{Data processing on the receiver side}
\begin{wrapfigure}[24]{r}{\textwidth/2}
\label{img:hand_model_workflow}
\includegraphics[width=\textwidth/2]{images/Rendering_workflow.png} 
\caption{Overview diagram of the processing steps for the hand model data on the client side}
\end{wrapfigure}
To ensure that the two data streams are separated and therefore distinguishable, each stream receives a dedicated listening port on the client side (Figure \ref{img:netzwerk_diagram}), makeing it possible to separate the UDP listeners to separate running threads in the background.The positional data will be encoded as comma-separated x and y coordinate sets. Data separation is done by a simple semicolon as a separator between the coordinate sets. After the string decoding, the extracted values are written into thread-safe variables.This allows the system to read current data without conflicting the writing process of the UPD threads.
\subsection{Stereoscopic calculations}
The positional data that is extracted in the UDP data processing only consists of 2D position data. To retrieve depth information for correct spatial placement of the marker, the image disparity of the two cameras can be utilized to calculate the object distance from the cameras.\cite{Tauer.2010}\newpage
Before diving into the mathematical concepts, some essential terms have to be explained.
These are:
\begin{itemize}
\item Inter-axial distance
\item Parallax
\item Zero-point plane
\end{itemize}
\subsubsection{Inter-axial distance}
Stereoscopic imaging systems are based off of our own human visual system. Therefore such a system always consists of at leas two cameras which are mounted in parallel. The distance difference between the two cameras is called the \textit{"inter-axial distance"} of the system. The usual distance for such systems is the mean distance of the human eyes, which corresponds to around 75mm. shifting the cameras closer together or further apart will change the resulting stereoscopic effect, where shifting the cameras closer together will enlarge objects and shifting the cameras further apart will make objects seem much smaller than they are. 
\subsubsection{Parallax}
Through the spearation distance of the cameras, a \textit{disparity} in the generated images will occur. This means that the objects in the picture of the right camera will have a different position in terms of perspective than the images that result from the left camera. This resulting disparity is used to generate the 3D effect when displaying the images. The term "\textit{parallax}" is used to describe the comparison of the two images. The two images can have a negative, positive or zero \textit{parallax}.
\subsubsection{Zero-point plane}
The\textit{ Zero-point plane} is a special theoretical plane in a stereoscopic system. The location of the \textit{Zero-point plane} defines the distance from the camera system at which the image points from both cameras coincide. As Figure \ref{img:stereo_diagram} shows, objects that lie before the \textit{zero-point plane}(closer to the camera) will have a negative \textit{parallax} and will appear closer to the viewer. On the opposite side, objects with positive \textit{parallax} will lie behind the zero point plane(further away from the camera) and therefore will appear further away.
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/Stereo_diagram.png}
\caption{Stereoscopic areas}
\label{img:stereo_diagram} 
\end{figure}
When setting up parallel stereoscopic setups,additional attention needs to be put in the camera placement before calculating depth form disparity. Horizontal misalignment of the cameras can lead to incorrect calculation results when calculating depth from disparity.
\subsection{Depth calculation}
With a correctly aligned system, one can calculate the distance of an object from the cameras using common trigonometric fundamentals\cite{JernejMrovlje.2008,YasirDawoodSalman.2017}.
\\
The position for the right camera will be denoted as $S_{R}$ and the position of the left camera will be denoted as $S_{L}$.The resulting distance of $S_{L}-S_{R}$ is the \textit{inter-axial distance} $B$ of the system. The view angle $\theta$ of the Camera, which should be the same for both cameras, will respectively be $\theta_{1}$ and $\theta_{2}$. From the camera positioning, the assumption of parallel optical axes is made. The angles of $\phi_{l},\phi_{r}$ describe the resulting angle between the optical axis of the camera and the object.
When looking at Figure\ref{stereo_Dimmensions}a),one can express that:\\
\begin{equation}
B= B_{1}+B_{2}=D\tan\varphi_{1}+D\tan\varphi_{2}
\end{equation}\\
Rearranging for \textit{D} gives us:\\
\begin{equation}
D = \frac{B}{\tan\varphi_{1}+\tan\varphi_{2}}
\end{equation}
\begin{figure}[H]
\includegraphics[width=\textwidth]{images/Stereo_Calc_4.JPG}
\caption{Dimmensions for calculation.Images from:\cite{JernejMrovlje.2008}}
\label{stereo_Dimmensions} 
\end{figure}
With Figure \ref{stereo_Dimmensions}b)-c) one can find that:
\begin{equation}
\frac{x_{1}}{\frac{x_{0}}{2}}=\frac{tan \varphi_1}{\tan(\frac{\varphi_{0}}{2})}
\end{equation}\\
\begin{equation}
\frac{-x_{2}}{\frac{x_{0}}{2}}=\frac{tan \varphi_2}{\tan(\frac{\varphi_{0}}{2})}
\end{equation}\\
Resulting in the calculation of \textit{D} to be expressible as:
\begin{equation}
D=\frac{Bx_0}{2\tan(\frac{\varphi_0}{2})(X_{L}-X_{R})}
\end{equation}\\
Where B is the distance between the cameras, $x_0$ being the horizontal width of the picture format in pixels, $\varphi_0$ representing the viewing angle of the camera and $(X_{L}-X_{R})$ representing the disparity between the two images in Pixels.
\subsection{Model position update}
Before a solution with the inverse kinematics algorithm can be calculated, the hand movement in real space has to be matched with that on the digital side. To achieve this, a further marker besides the finger markings is required. A sixth color marker has to be positioned on top of the hand to serve as a global position reference. The position and rotation difference between the last and current frame can then be calculated from the tracking values and then be applied to the whole model.
\subsection{Inverse Kinematics}
For the inverse kinematics calculation, the \textit{Fabrik} algorithm that is described in Section \ref{sec:fabrik_algorithm} was selected. It does not require complex matrix calculations and has a fast convergence rate. For the implementation of the algorithm on the "Master" side, the \textit{Caliko} Java framework\cite{Lansley.2016} is used.The usage of a Java framework provides a operating-system independent implementation of the solution. The library supplies all the needed classes and function to create \textit{kinematic chains} and \textit{structures} as well as the implementation of the algorithm itself. It furthermore ships a visualization package which utilizes the \textit{Lightweight Java Game Library 3} (LWJG3).The provided classes can be used optionally for debug visualization or even as a base for a more sophisticated visualization as it is possible to load further components for i.e. VR into LWJGL.\\\\
For the inverse kinematics calculation to work properly, the updated bone positions and the calculated target points from the finger tracking are needed.
The length of each bone for the hand model has to be measured manually for the first prototype.Further iterations should provide a more elaborated and maybe even automated calibration procedure. The movement of each bone in the kinematic chains has to be restricted by constraints to restrict the algorithms solutions to physiological possible hand poses. The values for finger constraints that are described in Section \ref{sec:Physiological_structure_of_the_human_hand} should provide sufficient starting values values. 